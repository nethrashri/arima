{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5ce48-e7ba-46c5-9c29-20eb0564d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# ---------------------\n",
    "# # 5. Transformer Model Implementation and Training\n",
    "# ---------------------\n",
    "\n",
    "# Define Positional Encoding\n",
    "def get_positional_encoding(sequence_length, d_model):\n",
    "    angle_rads = get_angles(np.arange(sequence_length)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "# Define the Transformer Block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        print(embed_dim // num_heads)\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Define the Token and Position Embedding\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Dense(embed_dim)\n",
    "        self.pos_emb = get_positional_encoding(sequence_length, embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        return x + self.pos_emb[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# Build the Transformer Model\n",
    "def build_transformer_model(sequence_length, feature_dim, embed_dim, num_heads, ff_dim, num_layers, dropout_rate=0.1):\n",
    "    inputs = layers.Input(shape=(sequence_length, feature_dim))\n",
    "    embedding_layer = TokenAndPositionEmbedding(sequence_length, feature_dim, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "        x = transformer_block(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    usage_minutes_head = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    usage_minutes_head = layers.Dense(1, name=\"usage_minutes_output\")(usage_minutes_head)\n",
    "    usage_minutes_output = layers.Dense(1, activation=\"linear\", name=\"usage_minutes_output\")(usage_minutes_head)\n",
    "    \n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=[usage_minutes_output]) # Priority_score_output\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Define model hyperparameters\n",
    "EMBED_DIM = 64  # Embedding size for each token\n",
    "NUM_HEADS = 4  # Number of attention heads\n",
    "FF_DIM = 128  # Hidden layer size in the feed-forward network\n",
    "NUM_LAYERS = 1  # Number of Transformer blocks\n",
    "\n",
    "# Get feature dimension\n",
    "feature_dim = X.shape[2]  # Number of features in input data\n",
    "\n",
    "# Build the model\n",
    "transformer_model = build_transformer_model(\n",
    "    sequence_length=3,  # Example sequence length\n",
    "    feature_dim=feature_dim,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d3efa-9c7a-44da-8744-9e8a966b1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "transformer_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'usage_minutes_output': 'mse',\n",
    "        # 'priority_score_output': 'mse'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'usage_minutes_output': 1.0,\n",
    "        # 'priority_score_output': 1.0\n",
    "    },\n",
    "    metrics={\n",
    "        'usage_minutes_output': 'mae',\n",
    "        # 'priority_score_output': 'mae'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9cbf8-de26-4404-98dd-7b5c8d46f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Define the log directory for TensorBoard\n",
    "log_dir = \"logs/model/transformer/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "print(y_val_usage_minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd959a-0901-4cb2-bbf8-6f6a17c8eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "history = transformer_model.fit(\n",
    "    X_train,\n",
    "    {\n",
    "        'usage_minutes_output': y_train_usage_minutes,\n",
    "        # 'priority_score_output': y_train_priority_score\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(\n",
    "        X_val,\n",
    "        {\n",
    "            'usage_minutes_output': y_val_usage_minutes,\n",
    "            # 'priority_score_output': y_val_priority_score\n",
    "        }\n",
    "    ),\n",
    "    callbacks=[tensorboard_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38113350-e54d-45ef-b291-61bbe4661660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814066a9-72b3-4c76-99d1-5b694186c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "usage_pred = transformer_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0caf3-0362-4b42-849c-cd1a1bb3af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(usage_pred.shape)\n",
    "print('**********')\n",
    "#print(priority_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cc3e6-aa07-4adc-8f20-908342e22ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 6. Model Evaluation\n",
    "# ================================\n",
    "\n",
    "# Load scalers for inverse transformation of predictions\n",
    "usage_minutes_scaler = joblib.load('usage_minutes_scaler.pkl')\n",
    "usage_minutes_predicted = usage_minutes_scaler.inverse_transform(usage_pred)\n",
    "actual_usage_minutes = usage_minutes_scaler.inverse_transform(y_val_usage_minutes.reshape(-1, 1))\n",
    "\n",
    "timestampScaler = joblib.load('time_scaler.pkl')\n",
    "actual_timestamp = timestampScaler.inverse_transform(timestamps_val2.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3f122-71e9-4aa8-9b38-29e66256fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(actual_usage_minutes, usage_minutes_predicted)  \n",
    "# Calculate Mean Absolute Error (MAE) between actual and predicted usage minutes\n",
    "\n",
    "print(mae)  \n",
    "# Print the MAE result\n",
    "\n",
    "usage_rmse = np.sqrt(mean_squared_error(actual_usage_minutes, usage_minutes_predicted))  \n",
    "# Calculate Root Mean Squared Error (RMSE) for usage minutes\n",
    "\n",
    "print(usage_rmse)  \n",
    "# Print the RMSE result\n",
    "\n",
    "usage_r2 = r2_score(actual_usage_minutes, usage_minutes_predicted)  \n",
    "# Calculate R2 score to evaluate the variance explained by the predictions\n",
    "\n",
    "print(usage_r2)  \n",
    "# Print the R2 score result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
